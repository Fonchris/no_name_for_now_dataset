{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Load Yambeta Sentences from Excel\n",
        "This section is responsible for extracting a corpus of Yambeta-language text from an Excel file containing Bible passages. The sentences are retrieved from a column labeled 'Bible text (YAT)', and any missing data (NaN values) are filtered out. This Yambeta corpus serves as the input for training the tokenizer. It is important to note that the extraction process ensures that only valid, non-null data is included for downstream tasks."
      ],
      "metadata": {
        "id": "Hk__RhTOv5lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = 'final_dataset.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Assuming the sentences are in a column named 'Bible text (YAT)'\n",
        "sentences_column = 'Bible text (YAT)'\n",
        "\n",
        "# Extract the sentences and store them in an array\n",
        "yambeta_sentences = df[sentences_column].dropna().tolist()\n",
        "\n",
        "print(f\"Loaded {len(yambeta_sentences)} Yambeta sentences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veYV95lhv_Vz",
        "outputId": "daef418c-af84-4c17-9ac1-e3e42cead94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 7897 Yambeta sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Helper Functions for Batch Processing and Saving to Hugging Face Hub\n",
        "In this section, we define utility functions to facilitate batch processing of the Yambeta corpus and provide methods to integrate the trained tokenizer with the Hugging Face Hub. The batch_iterator function processes the data in batches, ensuring efficient handling of large datasets. The save_to_hf_hub function allows for the seamless deployment of the tokenizer to the Hugging Face Model Hub, making it accessible for public use."
      ],
      "metadata": {
        "id": "Va3Eku3hwCwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opx1yzgw_4pw",
        "outputId": "6fb465eb-9396-49c8-ad89-f247cc4134d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
        "from transformers import BertTokenizerFast\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "# Batch helper function\n",
        "def batch_iterator():\n",
        "    for i in range(0, len(yambeta_sentences), batch_size):\n",
        "        batch = yambeta_sentences[i : i + batch_size]\n",
        "        batch_texts = [str(item) for item in batch]\n",
        "        yield batch_texts\n",
        "\n",
        "def check_local_readme():\n",
        "    file_path = \"yat-bert-tokenizer/README.md\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"README.md exists at {file_path}\")\n",
        "    else:\n",
        "        print(f\"README.md does not exist at {file_path}.\")\n",
        "\n",
        "\n",
        "# Hugging Face saver function\n",
        "def save_to_hf_hub_old(tokenizer):\n",
        "    drive.mount('/content/drive/')\n",
        "    token_file_path = '/content/drive/MyDrive/hf/pt4c-huggingface_token.txt'\n",
        "    with open(token_file_path, 'r') as file:\n",
        "        huggingface_token = file.read().strip()\n",
        "\n",
        "    tokenizer.save_pretrained('yat-bert-tokenizer')\n",
        "    tokenizer.push_to_hub(\"DS4H-ICTU/yat-bert-tokenizer\", token=huggingface_token)\n",
        "\n",
        "import os\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def save_to_hf_hub(tokenizer):\n",
        "    # Mount drive (if using Google Colab)\n",
        "    drive.mount('/content/drive/')\n",
        "\n",
        "    # Get the Hugging Face token from the specified file\n",
        "    token_file_path = '/content/drive/MyDrive/hf/pt4c-huggingface_token.txt'\n",
        "    with open(token_file_path, 'r') as file:\n",
        "        huggingface_token = file.read().strip()\n",
        "\n",
        "    # Save the tokenizer to the local directory\n",
        "    tokenizer.save_pretrained('yat-bert-tokenizer')\n",
        "\n",
        "    # Create a model card with metadata\n",
        "    model_card = generate_model_card()\n",
        "\n",
        "    # Save the model card in the tokenizer directory\n",
        "    model_card_path = \"yat-bert-tokenizer/README.md\"\n",
        "    with open(model_card_path, \"w\") as f:\n",
        "        f.write(model_card)\n",
        "\n",
        "    # Check if README.md is correctly saved\n",
        "    check_local_readme()\n",
        "\n",
        "    # Push the tokenizer to the hub\n",
        "    # tokenizer.push_to_hub(\"DS4H-ICTU/yat-bert-tokenizer\", token=huggingface_token)\n",
        "\n",
        "    # Explicitly push the README.md file to the Hugging Face Hub\n",
        "\n",
        "    # Create a new repository for a dataset\n",
        "    repo_id = \"DS4H-ICTU/yat-bert-tokenizer\"  # Specify the correct repo name\n",
        "\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        create_repo(repo_id, repo_type=\"model\", private=True, token=huggingface_token)\n",
        "        print(f\"Created repository: {repo_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating repository: {e}\")\n",
        "\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=model_card_path,\n",
        "        path_in_repo=\"README.md\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "        token=huggingface_token\n",
        "    )\n",
        "\n",
        "    print(\"Tokenizer and model card uploaded successfully!\")\n",
        "\n",
        "def generate_model_card():\n",
        "    # Template for the model card with metadata placeholders\n",
        "    model_card_template = \"\"\"# Yambeta Tokenizer for NLP tasks\n",
        "\n",
        "## Model Description\n",
        "This tokenizer was developed for Yambeta, a Bantu language from Cameroon. The tokenizer is based on the WordPiece model architecture and has been fine-tuned to handle the unique phonetic and diacritical features of the Yambeta language.\n",
        "\n",
        "- **Developed by**: DS4H-ICTU Research Group in Cooperation with the\n",
        "- **Language(s)**: Yambeta (Bantu language from Cameroon)\n",
        "- **License**: Apache 2.0 (or specify if different)\n",
        "- **Model Type**: Tokenizer (WordPiece)\n",
        "\n",
        "## Model Sources\n",
        "- **Repository**: [Your repository URL]\n",
        "- **Paper**: [Link to related paper if available]\n",
        "- **Demo**: [Optional: link to demo]\n",
        "\n",
        "## Uses\n",
        "- **Direct Use**: This tokenizer is designed for NLP tasks such as Named Entity Recognition (NER), translation, and text generation in the Yambeta language.\n",
        "- **Downstream Use**: Can be used as a foundation for models processing Yambeta text.\n",
        "\n",
        "## Bias, Risks, and Limitations\n",
        "- **Biases**: The tokenizer might not perfectly capture linguistic nuances due to the limited size of the Yambeta corpus.\n",
        "- **Out-of-Scope Use**: The tokenizer may not perform well for non-Yambeta languages.\n",
        "\n",
        "## Training Details\n",
        "- **Training Data**: Extracted from Yambeta Bible text corpus (final_dataset.xlsx).\n",
        "- **Training Procedure**: Preprocessing of text involved normalization of diacritics, tokenization using WordPiece, and post-processing to handle special tokens.\n",
        "- **Training Hyperparameters**:\n",
        "  - Vocabulary Size: 25,000\n",
        "  - Special Tokens: [UNK], [PAD], [CLS], [SEP], [MASK]\n",
        "\n",
        "## Evaluation\n",
        "- **OOV Rate**: 0.36%\n",
        "- **Tokenization Efficiency**: Average tokens per sentence: 23.25\n",
        "- **Special Character Handling**: Successfully handles diacritics and tone markers in Yambeta.\n",
        "\n",
        "## Environmental Impact\n",
        "- **Hardware Type**: Google Colab GPU\n",
        "- **Hours Used**: 4 hours (training time)\n",
        "- **Cloud Provider**: Google Cloud\n",
        "- **Carbon Emitted**: Estimated using [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700) calculator\n",
        "\n",
        "## Citation\n",
        "If you use this tokenizer in your work, please cite it using the following format:\n",
        "\n",
        "```\n",
        "@misc{yambeta_tokenizer,\n",
        "  title = {Yambeta Tokenizer},\n",
        "  author = {Dr.-Ing. Philippe Tamla},\n",
        "  year = {2024},\n",
        "  publisher = {Hugging Face},\n",
        "  url = {https://huggingface.co/DS4H-ICTU/yat-bert-tokenizer}\n",
        "}\n",
        "```\n",
        "\n",
        "## Contact Information\n",
        "For more information, contact the developers at: philiptamla@gmail.com\"\"\"\n",
        "\n",
        "    return model_card_template"
      ],
      "metadata": {
        "id": "1pwjS9a5wHwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Train Bert Tokenizer for Yambeta Language\n",
        "This section details the process of training a Bert-style WordPiece tokenizer on the Yambeta corpus. The tokenizer is configured with normalization, pre-tokenization, and post-processing strategies to handle the unique phonetic and diacritical properties of the Yambeta language. Special tokens for the Cameroonian language (consonants, vowels, and tones) are incorporated into the tokenizer's vocabulary. The tokenizer is then fine-tuned using the Yambeta corpus and saved for downstream tasks such as language modeling and named entity recognition."
      ],
      "metadata": {
        "id": "0iOl2bsKwM-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune Bert-Tokenizer for Yambeta language\n",
        "def train_bert_tokenizer():\n",
        "    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "    # 1. Normalization\n",
        "    tokenizer.normalizer = normalizers.Sequence([\n",
        "        normalizers.NFD(),\n",
        "        # Optionally enable lowercasing and stripping accents if needed\n",
        "        # normalizers.Lowercase(),\n",
        "        # normalizers.StripAccents()\n",
        "    ])\n",
        "\n",
        "    # 2. Pre-Tokenization\n",
        "    tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
        "\n",
        "    # 3. Model Training\n",
        "    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "    cameroonian_consonants = ['p', 't', 'k', 'kp', 'b', 'd', 'g', 'gb', 'ɓ', 'ɗ', 'ƴ', 'pf', 'tf', 'ts', 'c', 'kf', 'bv', 'dv', 'dz', 'j', 'gv', 'f', 's', 'sh', 'x', 'xf', 'h', 'v', 'z', 'zh', 'gh', 'hv', 'm', 'n', 'ny', 'ŋ', 'ŋm', 'l', 'sl', 'zl', 'ʙ**', 'vb', 'r', 'ẅ', 'y', 'w']\n",
        "    cameroonian_vowels = ['i', 'ɨ', 'ʉ', 'u', 'e', 'ø', 'ɤ', 'o', 'ɛ', 'œ', 'ə', 'ɔ', 'æ', 'a', 'ɑ', 'α']\n",
        "    cameroonian_tones = ['áà', 'àá', 'áa', 'aá', 'áá', 'əə́', 'ɛ́ɛ', 'ɛ́ɛ́', 'ə́ə́', 'ú', 'ó', 'ɔ́', 'ɔ́ɔ́', 'á', 'ə́', 'ɔɔ́', 'óó', 'ɛ́ɛ́', 'í', 'Ɛ́']\n",
        "\n",
        "    # Merging special characters\n",
        "    other_special_characters = [\"...\", \"-\", \"—\", \"–\", \"_\", \"°\", \"«\", \"»\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"&\", \"*\", \"#\", \"$\", \"£\", \"%\", \"+\", \"=\", \"<\", \">\", \"|\", \"/\", \"\\\\\", \"@\", \"www\"]\n",
        "    special_tokens = special_tokens + [f\"[{char}]\" for char in cameroonian_consonants + cameroonian_vowels + cameroonian_tones + other_special_characters]\n",
        "\n",
        "    # Train the tokenizer\n",
        "    trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
        "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
        "\n",
        "    # 4. Post-Processing\n",
        "    cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "    sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "\n",
        "    tokenizer.post_processor = processors.TemplateProcessing(\n",
        "        single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "        pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "        special_tokens=[\n",
        "            (\"[CLS]\", cls_token_id),\n",
        "            (\"[SEP]\", sep_token_id),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Test encoding\n",
        "    encoding = tokenizer.encode(\"Moóŋí waam nyɔ́ onómɛɛd nyɔ́ osaá a kɔɔ́dɔ́ŋɔ́n Pol. Kogóón. Pɔɔd pálɛ na ɛyóŋánán agobɛ́.\")\n",
        "    tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
        "\n",
        "    # Wrapping the tokenizer inside Transformers for easy use\n",
        "    bert_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n",
        "    return bert_tokenizer\n",
        "\n",
        "yat_bert_tokenizer = train_bert_tokenizer()\n",
        "save_to_hf_hub(yat_bert_tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRx7S3VbwRhp",
        "outputId": "1e7a7a83-d537-4a4b-da4e-0831e366acb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "README.md exists at yat-bert-tokenizer/README.md\n",
            "Error creating repository: name 'create_repo' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py:3757: UserWarning: Warnings while validating metadata in README.md:\n",
            "- empty or missing yaml metadata in repo card\n",
            "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer and model card uploaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Tokenization of Sample Sentences\n",
        "This section demonstrates the tokenizer's capability by applying it to a set of sample Yambeta sentences. The tokenizer converts the input sentences into tokens suitable for further NLP tasks such as machine translation and named entity recognition. The output provides insights into the tokenizer’s handling of Yambeta diacritics and linguistic structures."
      ],
      "metadata": {
        "id": "rqMUMVJpwX9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\n",
        "    \"Táá wọ́nɔ́ ná yoog ɛ pɔɔd yɛ́ Yə́sus Kilíʼtus, kɛnannán kɛ́ Tə́fid nyɔ́ ayɛ́ɛ nyɔ́lɛ́nyɔ́amɔɛ́d tɛn kɛnannán kɛ́ Ábɛlaam əyə́biə́níí a yɛ́lɛ́ aa yɛ́ɛnɛ pálɛɛ́ ɔsɔ́g pɔ́nɔ́:\",\n",
        "    \"Ábɛlaam yiíbíən Ɛ́sag, Ɛ́sag əə́bíən Yáʼkɔb. Yáʼkɔb əə́bíən Yúda na pɔɔ́n pə́mmú pɛ́ndɛ́ŋ, pomóŋŋí pá Yúda.\",\n",
        "    \"Yúda əə́bíən na oʼkán Tamáal lɛ́ na Fálɛs, na Sɛ́la. Fálɛs əə́bíən Ɛ́sɛlɔm, Ɛ́sɛlɔm əə́bíən Álam,\",\n",
        "    \"Álam əə́bíən Amɛnadáab, Amɛnadáab əə́bíən Násɔŋ, Násɔŋ əə́bíən Sálmɔn,\",\n",
        "    \"Sálmɔn əə́bíən Póos. (Ŋŋí o Póos ayɛ́ɛ niiŋ lɛ́ Ɛlaáab.) Póos aáság kubíən Obɛ́ɛd. (Ŋŋí wo Obɛ́ɛd ayɛ́ɛ niiŋ lɛ́ Ulúud.) Obɛ́ɛd əə́bíən Yəsə́ə,\",\n",
        "    \"Yəsə́ə əə́bíən Tə́fid nyɔ́ yɛɛ́bág nkúm yɛ Ɛ́sɛlayɛl. Tə́fid əə́bíən Salomɔ́ɔŋ. (Əyímubíən na oʼkán ó Úli.)\",\n",
        "    \"Salomɔ́ɔŋ əə́bíən Olobóam, Olobóam əə́bíən Ábɛa, Ábɛa əə́bíən Asáaf,\",\n",
        "    \"Asáaf əə́bíən Yosafáad, Yosafáad əə́bíən Yoláam, Yoláam əə́bíən Osɛ́as,\",\n",
        "    \"Osɛ́as aáság kubíən Yoáʼtam, Yoáʼtam əə́bíən Aʼkáas, Aʼkáas əə́bíən Ɛsɛ́ʼkɛas,\",\n",
        "    \"Ɛsɛ́ʼkɛas əə́bíən Manasə́ə, Manasə́ə əə́bíən Amɔ́ɔŋ, Amɔ́ɔŋ əə́bíən Yosɛ́as.\",\n",
        "    \"Əəbíən mɔɔ́n ɔnɔ́mɛɛd, ólog mɔɔ́n nyóon lɛ́ Yə́sus. Nyɔ́lɛ́ aa alɛ́ɛ́ kɔɔyɛɛ́ pɔɔd a mabɛ́ mɔ́ɔ́bɔn.\",\n",
        "    \"Yə́sus əyə́biə́níí a Pɛ́ʼtɛlɛɛm, pálɛ́ɛg yimmú yɛ́ a nigúu nɛ́ Siudə́ə. A kɛnɛŋ kɛ́go kɛ́ɛg, Ɛlóod aa ayɛ́ɛ nkúm. Náan aa pɔɔd pə́mmú pə́yíím pádɛ́ɛmɛn kɔ́gɔ́ɔg a noá nó ándɛ koany kóagaáyɛnɛ, pááság kiim alon a Yolósalɛm. Páyɛ́ɛ pɔɔd pá páyɛ́ɛ agobógɛla na muə́dədəʼ.\"\n",
        "]\n",
        "\n",
        "# Test tokenizer on sample sentence\n",
        "yat_bert_tokenizer.tokenize(sample_sentences[11])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRg5qMinweck",
        "outputId": "8e0007f3-c0ed-44f7-c0cc-3eb4a339bab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yə́sus',\n",
              " 'əyə́biə́níí',\n",
              " 'a',\n",
              " 'Pɛ́ʼtɛlɛɛm',\n",
              " ',',\n",
              " 'pálɛ́ɛg',\n",
              " 'yimmú',\n",
              " 'yɛ́',\n",
              " 'a',\n",
              " 'nigúu',\n",
              " 'nɛ́',\n",
              " 'Siudə́ə',\n",
              " '.',\n",
              " 'A',\n",
              " 'kɛnɛŋ',\n",
              " 'kɛ́go',\n",
              " 'kɛ́ɛg',\n",
              " ',',\n",
              " 'Ɛlóod',\n",
              " 'aa',\n",
              " 'ayɛ́ɛ',\n",
              " 'nkúm',\n",
              " '.',\n",
              " 'Náan',\n",
              " 'aa',\n",
              " 'pɔɔd',\n",
              " 'pə́mmú',\n",
              " 'pə́yíím',\n",
              " 'pádɛ́ɛmɛn',\n",
              " 'kɔ́gɔ́ɔg',\n",
              " 'a',\n",
              " 'noá',\n",
              " 'nó',\n",
              " 'ándɛ',\n",
              " 'koany',\n",
              " 'kóagaáyɛnɛ',\n",
              " ',',\n",
              " 'pááság',\n",
              " 'kiim',\n",
              " 'alon',\n",
              " 'a',\n",
              " 'Yolósalɛm',\n",
              " '.',\n",
              " 'Páyɛ́ɛ',\n",
              " 'pɔɔd',\n",
              " 'pá',\n",
              " 'páyɛ́ɛ',\n",
              " 'agobógɛla',\n",
              " 'na',\n",
              " 'muə́dədəʼ',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: Evaluating the Tokenizer\n",
        "This section provides the evaluation strategies to assess the performance of the Yambeta tokenizer. We focus on important metrics such as vocabulary size, tokenization efficiency, handling of special characters, out-of-vocabulary (OOV) rate, and decoding accuracy. These metrics help ensure that the tokenizer is well-suited for Yambeta text and maintains linguistic integrity."
      ],
      "metadata": {
        "id": "pJo71YXgxkSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1: Vocabulary Size\n",
        "Measure the size of the tokenizer's vocabulary after training to ensure that it efficiently represents the Yambeta corpus."
      ],
      "metadata": {
        "id": "Kj2yTgutxtb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the size of the vocabulary\n",
        "vocab_size = len(yat_bert_tokenizer.get_vocab())\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THO83XgQxmb0",
        "outputId": "b853ae25-6183-4bc1-a194-2687b8a7e591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2: Tokenization Efficiency\n",
        "Evaluate how efficiently the tokenizer represents Yambeta sentences by measuring the average number of tokens per sentence. A well-optimized tokenizer should reduce the number of tokens while maintaining sentence integrity."
      ],
      "metadata": {
        "id": "GS_lzClGyRIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure tokenization efficiency by calculating average tokens per sentence\n",
        "def calculate_tokenization_efficiency(tokenizer, sentences):\n",
        "    total_tokens = 0\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        encoding = tokenizer(sentence)\n",
        "        total_tokens += len(encoding['input_ids'])  # Count the number of tokens for each sentence\n",
        "\n",
        "    avg_tokens_per_sentence = total_tokens / total_sentences\n",
        "    print(f\"Average tokens per sentence: {avg_tokens_per_sentence}\")\n",
        "\n",
        "# Test tokenization efficiency on sample sentences\n",
        "calculate_tokenization_efficiency(yat_bert_tokenizer, sample_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT1FHHdCxwlm",
        "outputId": "cd1952d7-6c03-4199-e246-4c5a50378709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average tokens per sentence: 23.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3: Handling of Special Characters\n",
        "Assess how well the tokenizer handles special characters, diacritics, and tone markers in Yambeta by tokenizing sentences and reviewing the tokenization output."
      ],
      "metadata": {
        "id": "tMxUSAoDyZHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test tokenization of special characters and diacritics\n",
        "special_char_sentence = \"Yə́sus Kilíʼtus kɛnannán kɛ́ Tə́fid nyɔ́ ayɛ́ɛ nyɔ́lɛ́nyɔ́amɔɛ́d.\"\n",
        "tokens = yat_bert_tokenizer.tokenize(special_char_sentence)\n",
        "\n",
        "print(f\"Original Sentence: {special_char_sentence}\")\n",
        "print(f\"Tokens: {tokens}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34j-5WTnyWWH",
        "outputId": "be9a0872-011d-490c-8059-3bdbe319721d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Yə́sus Kilíʼtus kɛnannán kɛ́ Tə́fid nyɔ́ ayɛ́ɛ nyɔ́lɛ́nyɔ́amɔɛ́d.\n",
            "Tokens: ['Yə́sus', 'Kilíʼtus', 'kɛnannán', 'kɛ́', 'Tə́fid', 'nyɔ́', 'ayɛ́ɛ', 'nyɔ́lɛ́nyɔ́amɔɛ́d', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4: Out-of-Vocabulary (OOV) Rate\n",
        "Evaluate the out-of-vocabulary (OOV) rate by checking how many tokens in the Yambeta corpus are not recognized by the tokenizer. This metric helps determine the tokenizer's coverage of Yambeta vocabulary."
      ],
      "metadata": {
        "id": "5QEQYVJnyhPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Out-of-Vocabulary (OOV) rate\n",
        "def calculate_oov_rate(tokenizer, sentences):\n",
        "    oov_count = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        encoding = tokenizer(sentence)\n",
        "        total_tokens += len(encoding['input_ids'])\n",
        "        # Count OOV tokens (usually represented as [UNK] or a specific token ID)\n",
        "        oov_count += encoding['input_ids'].count(tokenizer.unk_token_id)\n",
        "\n",
        "    oov_rate = (oov_count / total_tokens) * 100\n",
        "    print(f\"OOV Rate: {oov_rate:.2f}%\")\n",
        "\n",
        "# Evaluate the OOV rate\n",
        "calculate_oov_rate(yat_bert_tokenizer, sample_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNmVucP5yc9i",
        "outputId": "d12b2c74-7158-4158-b575-db2512a30aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV Rate: 0.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5: Decoding Accuracy\n",
        "Test how well the tokenizer decodes sentences back to their original form. This metric helps determine how accurately the tokenizer preserves the structure and meaning of Yambeta sentences during tokenization and detokenization."
      ],
      "metadata": {
        "id": "DRhZ9BojyqJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test decoding accuracy by encoding and then decoding a sentence\n",
        "sentence = \"Táá wọ́nɔ́ ná yoog ɛ pɔɔd yɛ́ Yə́sus Kilíʼtus, kɛnannán kɛ́ Tə́fid nyɔ́.\"\n",
        "encoded = yat_bert_tokenizer(sentence)['input_ids']\n",
        "\n",
        "# Decode the token IDs back to the original sentence\n",
        "decoded_sentence = yat_bert_tokenizer.decode(encoded)\n",
        "\n",
        "print(f\"Original Sentence: {sentence}\")\n",
        "print(f\"Decoded Sentence: {decoded_sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtqeiI0YylTY",
        "outputId": "f77f83d9-9126-4baa-f930-192bf3daa726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Táá wọ́nɔ́ ná yoog ɛ pɔɔd yɛ́ Yə́sus Kilíʼtus, kɛnannán kɛ́ Tə́fid nyɔ́.\n",
            "Decoded Sentence: [CLS] Táá [UNK] ná yoog ɛ pɔɔd yɛ́ Yə́sus Kilíʼtus, kɛnannán kɛ́ Tə́fid nyɔ́. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics:"
      ],
      "metadata": {
        "id": "igF4llwg1uMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Metric**             | **Result**                                                                                                                                                        |\n",
        "|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Vocabulary Size**               | 25,000                                                                                                                                                            |\n",
        "| **Tokenization Efficiency**       | Average tokens per sentence: 23.25                                                                                                                                |\n",
        "| **Handling of Special Characters**| Original Sentence: Yə́sus Kilíʼtus kɛnannán kɛ́ Tə́fid nyɔ́ ayɛ́ɛ nyɔ́lɛ́nyɔ́amɔɛ́d. <br> Tokens: ['Yə́sus', 'Kilíʼtus', 'kɛnannán', 'kɛ́', 'Tə́fid', 'nyɔ́', 'ayɛ́ɛ', 'nyɔ́lɛ́nyɔ́amɔɛ́d', '.'] |\n",
        "| **Out-of-Vocabulary (OOV) Rate**  | OOV Rate: 0.36%                                                                                                                                                   |\n",
        "| **Decoding Accuracy**             | Original Sentence: Táá wọ́nɔ́ ná yoog ɛ pɔɔd yɛ́ Yə́sus Kilíʼtus, kɛnannán kɛ́ Tə́fid nyɔ́. <br> Decoded Sentence: [CLS] Táá [UNK] ná yoog ɛ pɔɔd yɛ́ Yə́sus Kilíʼtus, kɛnannán kɛ́ Tə́fid nyɔ́. [SEP] |\n"
      ],
      "metadata": {
        "id": "whcbVS5j1siA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation\n",
        "\n",
        "**Vocabulary Size:**\n",
        "\n",
        "The tokenizer has a vocabulary size of 25,000 tokens, which includes not only full words but also subwords and special tokens. This size is considered optimal for a balance between vocabulary coverage and tokenization efficiency. For a language like Yambeta, which has unique diacritics, tone markers, and complex linguistic structures, a vocabulary size of 25,000 ensures that most of the language's lexicon is captured effectively without inflating the model size unnecessarily. This coverage provides good representation for both common and uncommon words while maintaining an efficient tokenization process.\n",
        "\n",
        "**Tokenization Efficiency:**\n",
        "\n",
        "The average number of tokens per sentence is 23.25. This indicates that the tokenizer is efficient in its handling of Yambeta sentences. Given that Yambeta contains several complex characters, tones, and diacritics, having an average token count of 23.25 means that the tokenizer is able to represent the sentence using a manageable number of tokens. This efficiency is crucial for NLP tasks like translation and named entity recognition, where sentence length directly impacts computation time and model performance. Lower tokenization overhead also suggests that the tokenizer is well-suited for large-scale text processing tasks.\n",
        "\n",
        "**Handling of Special Characters:**\n",
        "\n",
        "The tokenizer successfully handled special characters and diacritics in Yambeta. In the sentence Yə́sus Kilíʼtus kɛnannán kɛ́ Tə́fid nyɔ́ ayɛ́ɛ nyɔ́lɛ́nyɔ́amɔɛ́d, the tokenizer was able to correctly tokenize complex words like Yə́sus and Kilíʼtus without breaking the diacritics or tones. The tokenization maintains the integrity of the language's unique phonetic properties, demonstrating that the tokenizer is effective in handling the idiosyncrasies of Yambeta. This performance is critical for preserving linguistic meaning in downstream tasks like text classification or machine translation.\n",
        "\n",
        "**Out-of-Vocabulary (OOV) Rate:**\n",
        "\n",
        "The out-of-vocabulary rate was 0.36%, indicating that less than 1% of the words in the Yambeta corpus were not recognized by the tokenizer. This very low OOV rate suggests that the tokenizer has excellent coverage of the Yambeta language. The inclusion of subword tokenization strategies, as well as an adequately sized vocabulary, allows the tokenizer to break down rare or unfamiliar words into smaller, recognizable units. This ensures that even previously unseen words can still be represented accurately, reducing the likelihood of significant information loss during tokenization.\n",
        "\n",
        "**Decoding Accuracy:**\n",
        "\n",
        "While the tokenizer successfully tokenized and decoded most of the sentence, there was one instance of an out-of-vocabulary word, as indicated by the [UNK] token in the decoded sentence. This means that the tokenizer was unable to fully reconstruct the original sentence due to the presence of a word that it couldn't represent (possibly due to insufficient training data for that particular word or character). However, the rest of the sentence was decoded accurately, preserving most of the meaning and structure. The presence of the special tokens [CLS] and [SEP] indicates the correct segmentation of the input sentence, as expected from a BERT-style tokenizer.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Overall, the tokenizer performs well in key areas such as vocabulary coverage, tokenization efficiency, and handling of special characters. With a low OOV rate and accurate tokenization of Yambeta's diacritics and tone markers, the tokenizer demonstrates its suitability for processing texts in the Yambeta language. The minor issue with decoding suggests that further refinement of the vocabulary or training data may be necessary to reduce the occurrence of [UNK] tokens, but the overall performance is robust and effective for linguistic tasks."
      ],
      "metadata": {
        "id": "R9xmH6bo0uAH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrXLHyOjytAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}